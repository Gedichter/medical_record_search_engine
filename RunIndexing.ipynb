{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RunIndexing\n",
    "\n",
    "This notebook performs indexing of all the medical reports in the database. It stores the results in index files that are subsequently used by the Backend notebook in order to enable efficient querying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run includes/imports.py\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "nlp = spacy.load('en', disable=['textcat', 'ner'])\n",
    "from includes.stringop import StringOp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/NOTEEVENTS.csv', dtype={'TEXT': str}, usecols = ['TEXT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('words/conditions.txt') as f:\n",
    "    wlist = f.readlines()\n",
    "    wlist = [str.lower(x.strip()) for x in wlist]\n",
    "    \n",
    "wordlist = []\n",
    "for w in wlist:\n",
    "    wordlist.append(w.split('/'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def full_query(base_token):\n",
    "    '''\n",
    "    Performs a parsing tree traversal, and returns the aggregated words.\n",
    "    base_token: the starting point.\n",
    "    '''\n",
    "    Q = [base_token]\n",
    "    results = []\n",
    "    while(len(Q)>0):\n",
    "        element = Q[0]\n",
    "        del Q[0]\n",
    "        #backward\n",
    "        if element.dep_ in ['pobj','amod','prep','conj','nsubj','neg'] and not element.head==base_token and not str.lower(element.head.text) in results:\n",
    "            Q.append(element.head)\n",
    "            results.append(str.lower(element.head.text))\n",
    "        #forward\n",
    "        for c in element.children:\n",
    "            if c.dep_ in ['amod','compound','nsubj','prep','pobj','advmod','det','neg'] and not str.lower(c.text) in results:\n",
    "                Q.append(c)\n",
    "                results.append(str.lower(c.text))\n",
    "    return results\n",
    "                \n",
    "def extract(text):\n",
    "    '''\n",
    "    Finds all occurrences of conditions present in the given full text, and all the related attributes by executing the query function.\n",
    "    Returns a dictionary that maps from condition to a dictionary of attributes -> number of occurrences.\n",
    "    text: the text to operate on.\n",
    "    '''\n",
    "    conditions = StringOp.find_condition(wordlist, text)\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    #create map of char index -> token\n",
    "    mp = {}\n",
    "    for i in range(1, len(doc)):\n",
    "        left = doc[i-1].idx\n",
    "        right = doc[i].idx\n",
    "        for j in range(left, right):\n",
    "            mp[j] = doc[i-1]\n",
    "        \n",
    "    results = {}\n",
    "    \n",
    "    for condition, char_index_matches in conditions.items():\n",
    "        num_cond_occurred = len(char_index_matches)\n",
    "        results[condition] = {}\n",
    "        for char_index in char_index_matches:\n",
    "            if not char_index in mp:\n",
    "                print('error index not found', char_index)\n",
    "                #print(mp)\n",
    "                continue\n",
    "            basetoken = mp[char_index]\n",
    "            att = full_query(basetoken)\n",
    "            for at in att:\n",
    "                if not at in results[condition]: results[condition][at] = 0\n",
    "                results[condition][at]+=1\n",
    "        results[condition]['-total-'] = num_cond_occurred\n",
    "    \n",
    "    return results\n",
    "    \n",
    "def run_indexing(path, df, selected_records=None):\n",
    "    '''\n",
    "    Runs the indexing process on the selected records sequentially (single-threaded).\n",
    "    path: the path in which to save the resulting index file.\n",
    "    df: the original dataframe.\n",
    "    selected_records: operates on the whole dataframe if None, otherwise just on the given indices.\n",
    "    '''\n",
    "    df_subset = df\n",
    "    if selected_records!=None:\n",
    "        df_subset = df.iloc[selected_records]\n",
    "    f = open(path,'w+')\n",
    "    for i in tqdm(range(df_subset.shape[0])): \n",
    "        med_text = df_subset.iloc[i]['TEXT']\n",
    "        sentences = StringOp.clean(med_text)\n",
    "        document = \" \".join(sentences)\n",
    "        res_dict = extract(document) \n",
    "        json.dump(res_dict,f)\n",
    "        f.write('\\n')\n",
    "    f.close()\n",
    "    \n",
    "def write_index_single(path, df): \n",
    "    '''\n",
    "    Helper function where individual processes operate on.\n",
    "    path: the path in which to save the resulting index file.\n",
    "    df: the dataframe to operate on.\n",
    "    '''\n",
    "    f = open(path,'w+')\n",
    "    for index in range(df.shape[0]): \n",
    "        sentences = StringOp.clean(df.iloc[index]['TEXT'])\n",
    "        document = \" \".join(sentences)\n",
    "        ddict = extract(document)\n",
    "        json.dump(ddict,f)\n",
    "        f.write('\\n')       \n",
    "        \n",
    "    f.close()\n",
    "    \n",
    "def run_indexing_parallel(path, df, num_cores, selected_records=None, verbose=False):\n",
    "    '''\n",
    "    Runs the indexing process on the selected records in parallel (multi-threaded).\n",
    "    path: the path in which to save the resulting index file.\n",
    "    df: the original dataframe.\n",
    "    num_cores: the number of parallel processes to use.\n",
    "    selected_records: operates on the whole dataframe if None, otherwise just on the given indices.\n",
    "    verbose: determines verbosity level.\n",
    "    '''\n",
    "    path_list = [path + 'index' + str(i) + '.json' for i in range(num_cores)]\n",
    "    df_subset = df\n",
    "    numrecords = df.shape[0]\n",
    "    if selected_records!=None:\n",
    "        df_subset = df.iloc[selected_records]\n",
    "        numrecords = len(selected_records)\n",
    "    range_list = [] \n",
    "    \n",
    "    #split up work for seperate processes\n",
    "    step = numrecords//num_cores\n",
    "    if numrecords%num_cores == 0:\n",
    "        range_list = [(i*step, (i+1)*step) for i in range(num_cores)]\n",
    "    else:\n",
    "        range_list = [(i*step, (i+1)*step) for i in range(num_cores-1)]\n",
    "        range_list.append(((num_cores-1)*step, numrecords))\n",
    "        \n",
    "    processes = []\n",
    "    for i in range(num_cores): #create processes operating on subsets of the data\n",
    "        lst = range_list[i]\n",
    "        df_sub = df_subset.iloc[lst[0]:lst[1]]\n",
    "        proc = mp.Process(target=write_index_single, args=(path_list[i], df_sub))\n",
    "        processes.append(proc)\n",
    "        del df_sub\n",
    " \n",
    "    if verbose:\n",
    "        start = timeit.default_timer()\n",
    "    for p in processes:\n",
    "        p.start()\n",
    "    print('processing started...')\n",
    "    \n",
    "    for p in processes:\n",
    "        p.join()\n",
    "    if verbose:\n",
    "        end = timeit.default_timer()\n",
    "        print(\"The running time is \", end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform single-threaded indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "run_indexing('index/index.json', data, None)\n",
    "print(time.time()-t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform multi-threaded indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "run_indexing_parallel('index/', data, 8, None)\n",
    "print(time.time()-t1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
